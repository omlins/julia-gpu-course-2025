{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4 - **Introduction to distributed parallelization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to:\n",
    "- understand the basic idea of distributed parallelization;\n",
    "- and learn about [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) and [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl) on the way.\n",
    "\n",
    "[*This content is distributed under BSD 3-Clause License. Authors: S. Omlin (CSCS, ETH Zurich).*](https://github.com/omlins/julia-gpu-course-2025/blob/main/LICENSE.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to get a good understanding of the concept of distributed computing. We want to focus on the understanding rather than the implementation and will, therefore, illustrate the essence of distributed computing with some single process examples; then, we will explore it further with [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl), which makes it almost trivial. ImplicitGlobalGrid.jl will enable our stencil-computation-based codes to run on multiple GPUs and CPUs in order to scale on modern multi-GPUs/CPU nodes, clusters and supercomputers. ImplicitGlobalGrid.jl implements the required inter-process communication using (GPU-aware) MPI. Each process handles one GPU or CPU (or CPU core)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the packages `ParallelStencil` and `Plots` (we will not use `ImplicitGlobalGrid` within the notebook as it requires MPI which is not always available in Jupyter services):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "] activate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ParallelStencil\n",
    "using ParallelStencil.FiniteDifferences2D\n",
    "@init_parallel_stencil(CUDA, Float64, 3) # or initialize with Threads, AMDGPU or Metal instead of CUDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the same 2-D heat diffusion solver as in the last part of the first *Data transfer optimisation notebook* (`2_datatransfer_optimisations.ipynb`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusion2D()\n",
    "    # Physics\n",
    "    lam      = 1.0                                          # Thermal conductivity\n",
    "    c0       = 2.0                                          # Heat capacity\n",
    "    lx, ly   = 10.0, 10.0                                   # Length of computational domain in dimension x and y\n",
    "\n",
    "    # Numerics\n",
    "    nx, ny   = 32*2, 32*2                                   # Number of gridpoints in dimensions x and y\n",
    "    nt       = 100                                          # Number of time steps\n",
    "    dx       = lx/(nx-1)                                    # Space step in x-dimension\n",
    "    dy       = ly/(ny-1)                                    # Space step in y-dimension\n",
    "\n",
    "    # Array initializations\n",
    "    T    = @zeros(nx, ny)                                   # Temperature\n",
    "    T2   = @zeros(nx, ny)                                   # 2nd array for Temperature\n",
    "    Ci   = @zeros(nx, ny)                                   # 1/Heat capacity\n",
    "\n",
    "    # Initial conditions\n",
    "    Ci .= 1/c0                                              # 1/Heat capacity (could vary in space)\n",
    "    T  .= Data.Array([100.0*exp(-(((ix-1)*dx-lx/4)/2)^2-(((iy-1)*dy-ly/2)/2)^2) for ix=1:size(T,1), iy=1:size(T,2)]) # Initialization of Gaussian temperature anomaly\n",
    "    T2 .= T;                                                # Assign also T2 to get correct boundary conditions.\n",
    "\n",
    "    # Time loop\n",
    "    dt  = min(dx^2,dy^2)/lam/maximum(Ci)/4.1                # Time step for 2D Heat diffusion\n",
    "    opts = (aspect_ratio=1, xlims=(1, nx), ylims=(1, ny), clims=(0.0, 10.0), c=:davos, xlabel=\"Lx\", ylabel=\"Ly\") # plotting options\n",
    "    @gif for it = 1:nt\n",
    "        @parallel diffusion2D_step!(T2, T, Ci, lam, dt, dx, dy)  # Diffusion time step.\n",
    "        heatmap(Array(T)'; opts...)\n",
    "        T, T2 = T2, T                                       # Swap the aliases T and T2 (does not perform any array copy)\n",
    "    end\n",
    "end\n",
    "\n",
    "@parallel function diffusion2D_step!(T2, T, Ci, lam, dt, dx, dy)\n",
    "    @inn(T2) = @inn(T) + dt*(lam*@inn(Ci)*(@d2_xi(T)/dx^2 + @d2_yi(T)/dy^2));\n",
    "    return\n",
    "end\n",
    "\n",
    "diffusion2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (periodic boundaries using halo)\n",
    "\n",
    "Distributed parallelization requires, in general, inter-process communication in order to produce correct results. Depending on the class of application, different communication patterns arise. Stencil-based applications are normally parallelized by decomposing the computational domain into multiple local problems where each process takes care of one of them. The processes are organized themselves in a topology that corresponds to topology of the local problems. In order to connect the local problems and have them form correctly the global problem, each process needs to receive from the neighboring processes the values of the boundaries of the local problem. In modelers' words, the boundary conditions of the local problems are defined by the processes that take care of the adjacent local problems. The following schema illustrates this in 2D:\n",
    "\n",
    "![implicit_global_grid.png](./figures/implicit_global_grid.png)\n",
    "\n",
    "Note that the local problem boundaries that are defined by the adjacent processes are called \"halos\" and the communication described above is called \"halo updates\".\n",
    "\n",
    "The concept of halo updates can be understood without having to be exposed to the full complexity of distributed parallelization: in a single process application, periodic boundaries can be implemented using halo updates. The following schema illustrates this in 2D:\n",
    "\n",
    "![periodic_boundaries.png](./figures/periodic_boundaries.png)\n",
    "\n",
    "The code below is identical to the one above, except that it contains now a call to `update_halo_periodx!`. Run the code and look at the visualization; then implement the function `update_halo_periodx!` in order to perform halo updates that will result in periodicity in x-dimension. Then, run the code again and note what has changed in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusion2D()\n",
    "    # Physics\n",
    "    lam      = 1.0                                          # Thermal conductivity\n",
    "    c0       = 2.0                                          # Heat capacity\n",
    "    lx, ly   = 10.0, 10.0                                   # Length of computational domain in dimension x and y\n",
    "\n",
    "    # Numerics\n",
    "    nx, ny   = 32*2, 32*2                                   # Number of gridpoints in dimensions x and y\n",
    "    nt       = 100                                          # Number of time steps\n",
    "    dx       = lx/(nx-1)                                    # Space step in x-dimension\n",
    "    dy       = ly/(ny-1)                                    # Space step in y-dimension\n",
    "\n",
    "    # Array initializations\n",
    "    T    = @zeros(nx, ny)                                   # Temperature\n",
    "    T2   = @zeros(nx, ny)                                   # 2nd array for Temperature\n",
    "    Ci   = @zeros(nx, ny)                                   # 1/Heat capacity\n",
    "\n",
    "    # Initial conditions\n",
    "    Ci .= 1/c0                                              # 1/Heat capacity (could vary in space)\n",
    "    T  .= Data.Array([100.0*exp(-(((ix-1)*dx-lx/4)/2)^2-(((iy-1)*dy-ly/2)/2)^2) for ix=1:size(T,1), iy=1:size(T,2)]) # Initialization of Gaussian temperature anomaly\n",
    "    T2 .= T;                                                # Assign also T2 to get correct boundary conditions.\n",
    "\n",
    "    # Time loop\n",
    "    dt  = min(dx^2,dy^2)/lam/maximum(Ci)/4.1                # Time step for 2D Heat diffusion\n",
    "    opts = (aspect_ratio=1, xlims=(1, nx), ylims=(1, ny), clims=(0.0, 10.0), c=:davos, xlabel=\"Lx\", ylabel=\"Ly\") # plotting options\n",
    "    @gif for it = 1:nt\n",
    "        @parallel diffusion2D_step!(T2, T, Ci, lam, dt, dx, dy)  # Diffusion time step.\n",
    "        update_halo_periodx!(T2)                                 # Update the halo of T2 locally such that we obtain periodic boundaries in x-dimension.\n",
    "        heatmap(Array(T)'; opts...)\n",
    "        T, T2 = T2, T                                       # Swap the aliases T and T2 (does not perform any array copy)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_halo_periodx!(A)\n",
    "    # ...\n",
    "    return\n",
    "end\n",
    "\n",
    "diffusion2D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (periodic boundaries using halo - faster)\n",
    "\n",
    "The function `update_halo_periodx!` calls now instead a kernel `kernel!` using ParallelStencil. Implement this kernel using `@parallel_indices`, taking into account that we will not launch a thread for each cell of array `A` but only for a subrange of it, which will map to the x-dimension halos that need to be updated (ranges argument `1:size(A,2)`). Run the code again and make sure that the visualization is identical as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_halo_periodx!(A)\n",
    "    @parallel (1:size(A,2)) kernel!(A)  # Call the kernel with the right ranges\n",
    "    return\n",
    "end\n",
    "\n",
    "@parallel_indices (iy) function kernel!(A::Data.Array)\n",
    "    #...\n",
    "    return\n",
    "end\n",
    "\n",
    "diffusion2D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (hide communication)\n",
    "\n",
    "For efficient scaling on supercomputers an important element is being able to hide communication between processes, e.g., halo updates, behind computation. In this example, we have only one process that \"communicates\" with itself. Nevertheless, we can also hide this \"communication\" in order to illustrate the concept. Therefore, use ParallelStencil's macro `@hide_communication` in order to overlap the computation with the halo update. Make sure that the simulation still produces the same result. The `@hide_communication` macro will automatically\n",
    "1) split the computations (`@parallel` call) into boundary region computations and inner point computations,\n",
    "2) launch both computations on different streams, and,\n",
    "3) as soon as the boundary region computations have finished, launch the communication (here `update_halo_periodx!`). \n",
    "\n",
    "Note that the communication is only hideable if launched on a non-blocking stream. If that is not the case it will still produce correct results but will not overlap.\n",
    "\n",
    "The macro `@hide_communication` takes two arguments\n",
    "1) a tuple, indicating the width of the boundary regions in each dimension (the boundaries must include (at least) all the data that is accessed in the communcation performed);\n",
    "2) a code block wich starts with one `@parallel` call to perform computations (for exceptions, see keyword `computation_calls`), followed by code to set boundary conditions and to perform communication (as e.g. update_halo! from the package ImplicitGlobalGrid).\n",
    "\n",
    "More information can be obtained typing `?@hide_communication`. Here is an example for hiding communication in 3D using the macro:\n",
    "```julia\n",
    "@hide_communication (32, 2, 2) begin\n",
    "    @parallel diffusion3D_step!(T2, Te Ci, lam, dt, dx, dy, dz);\n",
    "    update_halo!(T2);\n",
    "end\n",
    "```\n",
    "Note that in x-dimension we typically want to use a boundary regions with of 32 or at least 16 cells, in order to access the data also in the boundary regions computations in a contiguous fashion, leading to good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function diffusion2D()\n",
    "    # Physics\n",
    "    lam      = 1.0                                          # Thermal conductivity\n",
    "    c0       = 2.0                                          # Heat capacity\n",
    "    lx, ly   = 10.0, 10.0                                   # Length of computational domain in dimension x and y\n",
    "\n",
    "    # Numerics\n",
    "    nx, ny   = 32*2, 32*2                                   # Number of gridpoints in dimensions x and y\n",
    "    nt       = 100                                          # Number of time steps\n",
    "    dx       = lx/(nx-1)                                    # Space step in x-dimension\n",
    "    dy       = ly/(ny-1)                                    # Space step in y-dimension\n",
    "\n",
    "    # Array initializations\n",
    "    T    = @zeros(nx, ny)                                   # Temperature\n",
    "    T2   = @zeros(nx, ny)                                   # 2nd array for Temperature\n",
    "    Ci   = @zeros(nx, ny)                                   # 1/Heat capacity\n",
    "\n",
    "    # Initial conditions\n",
    "    Ci .= 1/c0                                              # 1/Heat capacity (could vary in space)\n",
    "    T  .= Data.Array([100.0*exp(-(((ix-1)*dx-lx/4)/2)^2-(((iy-1)*dy-ly/2)/2)^2) for ix=1:size(T,1), iy=1:size(T,2)]) # Initialization of Gaussian temperature anomaly\n",
    "    T2 .= T;                                                # Assign also T2 to get correct boundary conditions.\n",
    "\n",
    "    # Time loop\n",
    "    dt  = min(dx^2,dy^2)/lam/maximum(Ci)/4.1                # Time step for 2D Heat diffusion\n",
    "    opts = (aspect_ratio=1, xlims=(1, nx), ylims=(1, ny), clims=(0.0, 10.0), c=:davos, xlabel=\"Lx\", ylabel=\"Ly\") # plotting options\n",
    "    @gif for it = 1:nt\n",
    "        #...\n",
    "            @parallel diffusion2D_step!(T2, T, Ci, lam, dt, dx, dy)  # Diffusion time step.\n",
    "            update_halo_periodx!(T2)                                 # Update the halo of T2 locally such that we obtain periodic boundaries in x-dimension.\n",
    "        #...\n",
    "        heatmap(Array(T)'; opts...)\n",
    "        T, T2 = T2, T                                       # Swap the aliases T and T2 (does not perform any array copy)\n",
    "    end\n",
    "end\n",
    "\n",
    "diffusion2D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful hiding of communication behind computation can be verified, e.g., using Nvdia Nsigth Systems. Here is a screenshot showing overlap in a real world application using ParallelStencil and ImplicitGlobalGrid:\n",
    "![overlap.png](./figures/overlap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (multi-GPUs/CPU using ImplicitGlobalGrid)\n",
    "\n",
    "Let us now consider the following multi-GPUs/CPU 3-D heat diffusion solver, which uses ImplicitGlobalGrid for distributed parallelization and ParallelStencil for parallelization within a GPU or CPU. The communication - calls to `update_halo!` - is hidden behind computation using ParallelStencil's `@hide_communication` macro. The flag `USE_GPU` allows to switch between running on GPU or CPU. The code includes a straightforward in-situ visualization: every thousand iterations the temperature data from each local problem is gathered on the process with rank 0 and incorporated into a GIF and a MP4 visualization.\n",
    "\n",
    "You can find the code also in the file `diffusion3D_IGG.jl`. Run the code three times: with 1, 2 and 4 processes; look at the visualization after each run. Describe what you see in the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const USE_GPU = true\n",
    "using ImplicitGlobalGrid, Plots\n",
    "using ParallelStencil\n",
    "using ParallelStencil.FiniteDifferences3D\n",
    "@static if USE_GPU\n",
    "    @init_parallel_stencil(CUDA, Float64, 3);\n",
    "else\n",
    "    @init_parallel_stencil(Threads, Float64, 3);\n",
    "end\n",
    "\n",
    "@parallel function diffusion3D_step!(T2, T, Ci, lam, dt, _dx, _dy, _dz)\n",
    "    @inn(T2) = @inn(T) + dt*(lam*@inn(Ci)*(@d2_xi(T)*_dx^2 + @d2_yi(T)*_dy^2 + @d2_zi(T)*_dz^2));\n",
    "    return\n",
    "end\n",
    "\n",
    "function diffusion3D()\n",
    "# Physics\n",
    "lam        = 1.0;                                       # Thermal conductivity\n",
    "cp_min     = 1.0;                                       # Minimal heat capacity\n",
    "lx, ly, lz = 10.0, 10.0, 10.0;                          # Length of computational domain in dimension x, y and z\n",
    "\n",
    "# Numerics\n",
    "nx, ny, nz = 256, 256, 256;                             # Number of gridpoints in dimensions x, y and z\n",
    "nt         = 100000;                                    # Number of time steps\n",
    "me, dims   = init_global_grid(nx, ny, nz);\n",
    "dx         = lx/(nx_g()-1);                             # Space step in dimension x\n",
    "dy         = ly/(ny_g()-1);                             # ...        in dimension y\n",
    "dz         = lz/(nz_g()-1);                             # ...        in dimension z\n",
    "_dx, _dy, _dz = 1.0/dx, 1.0/dy, 1.0/dz;\n",
    "\n",
    "# Array initializations\n",
    "T   = @zeros(nx, ny, nz);\n",
    "T2  = @zeros(nx, ny, nz);\n",
    "Ci  = @zeros(nx, ny, nz);\n",
    "\n",
    "# Initial conditions (heat capacity and temperature with two Gaussian anomalies each)\n",
    "Ci .= 1.0./( cp_min .+ Data.Array([5*exp(-((x_g(ix,dx,Ci)-lx/1.5))^2-((y_g(iy,dy,Ci)-ly/2))^2-((z_g(iz,dz,Ci)-lz/1.5))^2) +\n",
    "                                   5*exp(-((x_g(ix,dx,Ci)-lx/3.0))^2-((y_g(iy,dy,Ci)-ly/2))^2-((z_g(iz,dz,Ci)-lz/1.5))^2) for ix=1:size(T,1), iy=1:size(T,2), iz=1:size(T,3)]) )\n",
    "T  .= Data.Array([100*exp(-((x_g(ix,dx,T)-lx/2)/2)^2-((y_g(iy,dy,T)-ly/2)/2)^2-((z_g(iz,dz,T)-lz/3.0)/2)^2) +\n",
    "                   50*exp(-((x_g(ix,dx,T)-lx/2)/2)^2-((y_g(iy,dy,T)-ly/2)/2)^2-((z_g(iz,dz,T)-lz/1.5)/2)^2) for ix=1:size(T,1), iy=1:size(T,2), iz=1:size(T,3)])\n",
    "T2 .= T;                                                # Assign also T2 to get correct boundary conditions.\n",
    "\n",
    "# Preparation of visualisation\n",
    "gr()\n",
    "ENV[\"GKSwstype\"]=\"nul\"\n",
    "anim = Animation();\n",
    "nx_v = (nx-2)*dims[1];\n",
    "ny_v = (ny-2)*dims[2];\n",
    "nz_v = (nz-2)*dims[3];\n",
    "T_v  = zeros(nx_v, ny_v, nz_v);\n",
    "T_nohalo = zeros(nx-2, ny-2, nz-2);\n",
    "\n",
    "# Time loop\n",
    "dt = min(dx^2,dy^2,dz^2)*cp_min/lam/8.1;                                                  # Time step for the 3D Heat diffusion\n",
    "for it = 1:nt\n",
    "    if (it == 11) tic(); end                                                              # Start measuring time.\n",
    "    if mod(it, 1000) == 1                                                                 # Visualize only every 1000th time step\n",
    "        T_nohalo .= Array(T[2:end-1,2:end-1,2:end-1]);                                    # Copy data to CPU removing the halo.\n",
    "        gather!(T_nohalo, T_v)                                                            # Gather data on process 0 (could be interpolated/sampled first)\n",
    "        if (me==0) heatmap(transpose(T_v[:,ny_v÷2,:]), aspect_ratio=1); frame(anim); end  # Visualize it on process 0.\n",
    "    end\n",
    "    @hide_communication (32, 2, 2) begin\n",
    "        @parallel diffusion3D_step!(T2, T, Ci, lam, dt, _dx, _dy, _dz);\n",
    "        update_halo!(T2);\n",
    "    end\n",
    "    T, T2 = T2, T;\n",
    "end\n",
    "time_s = toc()\n",
    "\n",
    "# Performance\n",
    "A_eff = (2*1+1)*1/1e9*nx*ny*nz*sizeof(Data.Number);      # Effective main memory access per iteration [GB] (Lower bound of required memory access: T has to be read and written: 2 whole-array memaccess; Ci has to be read: : 1 whole-array memaccess)\n",
    "t_it  = time_s/(nt-10);                                  # Execution time per iteration [s]\n",
    "T_eff = A_eff/t_it;                                      # Effective memory throughput [GB/s]\n",
    "if (me==0) println(\"time_s=$time_s T_eff=$T_eff\"); end\n",
    "\n",
    "# Postprocessing\n",
    "if (me==0) gif(anim, \"diffusion3D.gif\", fps = 15) end    # Create a gif movie on process 0.\n",
    "if (me==0) mp4(anim, \"diffusion3D.mp4\", fps = 15) end    # Create a mp4 movie on process 0.\n",
    "finalize_global_grid();                                  # Finalize the implicit global grid\n",
    "end\n",
    "\n",
    "diffusion3D()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (multi-GPUs/CPU using ImplicitGlobalGrid - periodicity)\n",
    "\n",
    "In the call to `init_global_grid` add the keyword argument `periodx=1` (`0` and `1` are used for `true` and `false` in order to be consistent with traditional MPI; in the next release `true` and `false` will also be valid). When you run it on a single process, then the call to `update_halo!` will do the analog of what you implemented in task 1 and 2! The feature will of course also work when you run it with more than one process. Run the code again and look at the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (multi-GPUs/CPU using ImplicitGlobalGrid - effect of halo updates)\n",
    "\n",
    "To finish this introduction on distributed parallelization, we want to yet further improve our understanding of halo updates. When halo updates are correctly implemented, their effect is not explicitly visible in a standard visualization. One way to understand their effect is to observe what happens when they are missing. Therefore, remove now the call to `update_halo!` (you also need to remove the `@hide_communication` macro call to do so).\n",
    "\n",
    "Run the code three times: with 1, 2 and 4 processes; look at the visualization after each run. Describe what you see in the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! This completes the introduction to distributed parallelization. You should now have a good understanding of the concept of distributed parallelization. Furthermore, you are now able to develop stencil-based applications that achieve high performance and near ideal scaling on the largest GPU Supercomputers! Here are the results of a scaling experiment on the new Alps supercomputer at CSCS in Switzerland:\n",
    "\n",
    "![julia_gpu_par_eff_CI95.png](./figures/julia_gpu_par_eff_CI95.png)\n",
    "\n",
    "You can observe that the 3-D heat diffusion solver implemented with ImplicitGlobalGrid and ParallelStencil achieved a parallel efficiency of 93% on 8000 GPUs. Note that with real world applications we can achieve an even better parallel efficiency.\n",
    "\n",
    "To learn more about ImplicitGlobalGrid, check out its [documentation](https://github.com/eth-cscs/ImplicitGlobalGrid.jl). Note in particular that ImplicitGlobalGrid handles also automatically staggered grids and enables larger halos for larger stencils. To get started with the development of stencil based applications check out the [multi-xPU miniapps](https://github.com/omlins/ParallelStencil.jl/tree/main?tab=readme-ov-file#concise-singlemulti-xpu-miniapps) in the ParallelStencil documentation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
